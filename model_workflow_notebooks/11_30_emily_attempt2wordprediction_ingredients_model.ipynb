{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NUnxMaFp392"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Followed this tutorial: https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe\n",
        "This is pretty basic next-word prediction from training on the NERs as sentences, it doesn't really use the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "36rS5hUZp395"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd \"/content/drive/My Drive/dis study abroad/Artificial Neural Networks\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxLNTawN0KVa",
        "outputId": "9f1d0ec6-937f-4d04-88e1-f3bf2410ef0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/dis study abroad/Artificial Neural Networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr8ULCp50XVB",
        "outputId": "91cb09f7-9356-43b0-bb2d-61cb772be2de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emily_ingredients_model.ipynb     full_dataset.csv.zip     NLG_cleaning.ipynb\n",
            "flavor_graph.ipynb                ingredients_model.ipynb  nodes_191120.csv\n",
            "FlavorGraph_NodeEmbedding.pickle  ner_embeddings.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uw77sTa5p396"
      },
      "outputs": [],
      "source": [
        "# open and load data from pickle file\n",
        "file = open('FlavorGraph_NodeEmbedding.pickle', 'rb')\n",
        "\n",
        "# keys are strings for node_id, value is embedding\n",
        "data = pickle.load(file)\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GyQwK9fvp396"
      },
      "outputs": [],
      "source": [
        "# load dataframe of node_ids and ingredients\n",
        "df = pandas.read_csv('nodes_191120.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HzHnkg0Lp397"
      },
      "outputs": [],
      "source": [
        "# just get the embeddings for ingredients\n",
        "ing_embeddings = {}\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df.loc[i, 'node_type'] == \"ingredient\":\n",
        "        # map the name of the ingredient to the embedding\n",
        "        ing_embeddings[df.loc[i, 'name']] = data[str(df.loc[i, 'node_id'])]\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pickle5\n",
        "import pickle5 as pickle"
      ],
      "metadata": {
        "id": "QZPrP1Ih18mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6c691b-3e50-4a7b-b9ee-f12777a243ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w_9rEEf7p397"
      },
      "outputs": [],
      "source": [
        "# open and load data from pickle file\n",
        "ner_embeddings_file = open('ner_embeddings.pkl', 'rb')\n",
        "\n",
        "# keys are strings for node_id, value is embedding\n",
        "ner_embeddings = pickle.load(ner_embeddings_file)\n",
        "\n",
        "ner_embeddings_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0e8tc86Tp398"
      },
      "outputs": [],
      "source": [
        "subset= ner_embeddings.sample(n=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "miCVkkYHp398",
        "outputId": "b1cadef8-e5b8-42d5-a801-84a57ea0acc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       NER  \\\n",
              "813442   [egg, oil, sugar, vanilla, zucchini, pineapple...   \n",
              "2039943            [flour, yeast, sugar, salt, oil, water]   \n",
              "1103950  [almond, bacon, cheddar_cheese, mayonnaise, on...   \n",
              "2188446  [margarine, applesauce, flour, oat, sugar, bak...   \n",
              "2163321  [chicken_broth, baby_carrot, green_bean, cabba...   \n",
              "...                                                    ...   \n",
              "1433791  [sugar, sake, mirin, soy_sauce, fresh_ginger, ...   \n",
              "1439435            [egg, sugar, cornstarch, milk, vanilla]   \n",
              "1423716  [butter, cream_cheese, vanilla, kosher_salt, p...   \n",
              "1819926  [cocoa, honey, margarine, splenda_sugar_substi...   \n",
              "186542                           [margarine, flour, cream]   \n",
              "\n",
              "                                                embeddings  num_ingredients  \\\n",
              "813442   [[0.25692862, 0.08700464, 0.23288268, 0.169938...               12   \n",
              "2039943  [[-0.31055188, 0.15737705, -0.012394806, 0.129...                6   \n",
              "1103950  [[0.18312205, -0.06159898, -0.2872927, 0.24822...                6   \n",
              "2188446  [[-0.10213083, -0.28854743, -0.16905284, 0.134...               11   \n",
              "2163321  [[0.11032726, -0.22686784, -0.120354585, -0.00...                6   \n",
              "...                                                    ...              ...   \n",
              "1433791  [[0.24356885, 0.20317887, -0.023231847, 0.0660...                7   \n",
              "1439435  [[0.25692862, 0.08700464, 0.23288268, 0.169938...                5   \n",
              "1423716  [[-0.4366374, 0.026041824, 0.15680166, 0.11945...                8   \n",
              "1819926  [[0.0020710647, 0.017117582, -0.29813024, 0.25...                6   \n",
              "186542   [[-0.10213083, -0.28854743, -0.16905284, 0.134...                3   \n",
              "\n",
              "                 start_ingredient  \\\n",
              "813442                        egg   \n",
              "2039943                     yeast   \n",
              "1103950            cheddar_cheese   \n",
              "2188446                  cinnamon   \n",
              "2163321                   cabbage   \n",
              "...                           ...   \n",
              "1433791              fresh_ginger   \n",
              "1439435                cornstarch   \n",
              "1423716                      milk   \n",
              "1819926  splenda_sugar_substitute   \n",
              "186542                      cream   \n",
              "\n",
              "                               embedding_matrix_index_list  \\\n",
              "813442   [2052, 4155, 5769, 6206, 6652, 4519, 2242, 276...   \n",
              "2039943               [2242, 6600, 5769, 5228, 4155, 6355]   \n",
              "1103950                  [79, 252, 1015, 3845, 4175, 5228]   \n",
              "2188446  [3806, 164, 2242, 4146, 5769, 277, 1285, 4140,...   \n",
              "2163321                 [1080, 221, 2792, 767, 5271, 2611]   \n",
              "...                                                    ...   \n",
              "1433791         [5769, 5207, 3979, 5601, 2335, 6355, 1529]   \n",
              "1439435                     [2052, 5769, 1529, 3926, 6206]   \n",
              "1423716     [727, 1582, 6206, 3380, 4719, 3926, 913, 3926]   \n",
              "1819926               [1338, 3084, 3806, 5665, 3822, 3929]   \n",
              "186542                                  [3806, 2242, 1581]   \n",
              "\n",
              "         start_ingredient_index  \n",
              "813442                     2052  \n",
              "2039943                    6600  \n",
              "1103950                    1015  \n",
              "2188446                    1285  \n",
              "2163321                     767  \n",
              "...                         ...  \n",
              "1433791                    2335  \n",
              "1439435                    1529  \n",
              "1423716                    3926  \n",
              "1819926                    5665  \n",
              "186542                     1581  \n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67731080-8ca0-40c8-a55d-5a55b80f2219\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NER</th>\n",
              "      <th>embeddings</th>\n",
              "      <th>num_ingredients</th>\n",
              "      <th>start_ingredient</th>\n",
              "      <th>embedding_matrix_index_list</th>\n",
              "      <th>start_ingredient_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>813442</th>\n",
              "      <td>[egg, oil, sugar, vanilla, zucchini, pineapple...</td>\n",
              "      <td>[[0.25692862, 0.08700464, 0.23288268, 0.169938...</td>\n",
              "      <td>12</td>\n",
              "      <td>egg</td>\n",
              "      <td>[2052, 4155, 5769, 6206, 6652, 4519, 2242, 276...</td>\n",
              "      <td>2052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2039943</th>\n",
              "      <td>[flour, yeast, sugar, salt, oil, water]</td>\n",
              "      <td>[[-0.31055188, 0.15737705, -0.012394806, 0.129...</td>\n",
              "      <td>6</td>\n",
              "      <td>yeast</td>\n",
              "      <td>[2242, 6600, 5769, 5228, 4155, 6355]</td>\n",
              "      <td>6600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103950</th>\n",
              "      <td>[almond, bacon, cheddar_cheese, mayonnaise, on...</td>\n",
              "      <td>[[0.18312205, -0.06159898, -0.2872927, 0.24822...</td>\n",
              "      <td>6</td>\n",
              "      <td>cheddar_cheese</td>\n",
              "      <td>[79, 252, 1015, 3845, 4175, 5228]</td>\n",
              "      <td>1015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2188446</th>\n",
              "      <td>[margarine, applesauce, flour, oat, sugar, bak...</td>\n",
              "      <td>[[-0.10213083, -0.28854743, -0.16905284, 0.134...</td>\n",
              "      <td>11</td>\n",
              "      <td>cinnamon</td>\n",
              "      <td>[3806, 164, 2242, 4146, 5769, 277, 1285, 4140,...</td>\n",
              "      <td>1285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2163321</th>\n",
              "      <td>[chicken_broth, baby_carrot, green_bean, cabba...</td>\n",
              "      <td>[[0.11032726, -0.22686784, -0.120354585, -0.00...</td>\n",
              "      <td>6</td>\n",
              "      <td>cabbage</td>\n",
              "      <td>[1080, 221, 2792, 767, 5271, 2611]</td>\n",
              "      <td>767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433791</th>\n",
              "      <td>[sugar, sake, mirin, soy_sauce, fresh_ginger, ...</td>\n",
              "      <td>[[0.24356885, 0.20317887, -0.023231847, 0.0660...</td>\n",
              "      <td>7</td>\n",
              "      <td>fresh_ginger</td>\n",
              "      <td>[5769, 5207, 3979, 5601, 2335, 6355, 1529]</td>\n",
              "      <td>2335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439435</th>\n",
              "      <td>[egg, sugar, cornstarch, milk, vanilla]</td>\n",
              "      <td>[[0.25692862, 0.08700464, 0.23288268, 0.169938...</td>\n",
              "      <td>5</td>\n",
              "      <td>cornstarch</td>\n",
              "      <td>[2052, 5769, 1529, 3926, 6206]</td>\n",
              "      <td>1529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423716</th>\n",
              "      <td>[butter, cream_cheese, vanilla, kosher_salt, p...</td>\n",
              "      <td>[[-0.4366374, 0.026041824, 0.15680166, 0.11945...</td>\n",
              "      <td>8</td>\n",
              "      <td>milk</td>\n",
              "      <td>[727, 1582, 6206, 3380, 4719, 3926, 913, 3926]</td>\n",
              "      <td>3926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819926</th>\n",
              "      <td>[cocoa, honey, margarine, splenda_sugar_substi...</td>\n",
              "      <td>[[0.0020710647, 0.017117582, -0.29813024, 0.25...</td>\n",
              "      <td>6</td>\n",
              "      <td>splenda_sugar_substitute</td>\n",
              "      <td>[1338, 3084, 3806, 5665, 3822, 3929]</td>\n",
              "      <td>5665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186542</th>\n",
              "      <td>[margarine, flour, cream]</td>\n",
              "      <td>[[-0.10213083, -0.28854743, -0.16905284, 0.134...</td>\n",
              "      <td>3</td>\n",
              "      <td>cream</td>\n",
              "      <td>[3806, 2242, 1581]</td>\n",
              "      <td>1581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67731080-8ca0-40c8-a55d-5a55b80f2219')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67731080-8ca0-40c8-a55d-5a55b80f2219 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67731080-8ca0-40c8-a55d-5a55b80f2219');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "subset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get processed data \n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Next, we tokenize the data by creating a tokenizer object and fitting it to the cleaned data. \n",
        "# Tokenizer vectorizes the text corpus, turning it into a dictionary of words and their indexes.\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "corpus = subset['NER']\n",
        "\n",
        "# For each review, we prepare n-grams to use as input sequences for training.\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "input_sequences = []\n",
        "\n",
        "for ingred_list in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([ingred_list])[0]\n",
        "\t# print(token_list)\n",
        "  # print(ingred_list)\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n"
      ],
      "metadata": {
        "id": "50btGFmr3e_3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingred_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtDwSWTuFkM1",
        "outputId": "280033f7-b41a-4816-b2e6-c2f18bbeaea4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['margarine', 'flour', 'cream']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9frqev9FmyM",
        "outputId": "78b9863d-ca51-4b89-ccd7-1f28e7970b20"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 4, 76]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we pad sequenced data and define predictors and labels. \n",
        "# We use predictors to guess what is the next word in a sequence and labels \n",
        "# to correct the model’s predictions.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow.keras.utils as ku\n",
        "\n",
        "# pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "metadata": {
        "id": "Vax1ILUn4wkg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "myUUfiX-46Y2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 240, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "WtyGcAEI498M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "# \tdef on_epoch_end(self, epoch, logs={}):\n",
        "# \t\tif(logs.get('accuracy')>0.93):\n",
        "# \t\t\tprint(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "# \t\t\tself.model.stop_training = True\n",
        "\n",
        "# callbacks = myCallback()\n",
        "\n",
        "# the tutorial says that accuracy definitely gets better over time, they ran for 300 epochs but it just takes forever\n",
        "# and we need to balance between the size of the subset vs the # of epochs so this is not a lot of train time relatively\n",
        "# we could run it for longer once we get models we actually like\n",
        "history = model.fit(predictors, label, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dArw-ceO5BVJ",
        "outputId": "180ca6bb-ffb6-4f49-be68-ab20a6934efd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "194/194 [==============================] - 51s 218ms/step - loss: 5.8964 - accuracy: 0.0641\n",
            "Epoch 2/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 5.2203 - accuracy: 0.0647\n",
            "Epoch 3/100\n",
            "194/194 [==============================] - 32s 164ms/step - loss: 5.0656 - accuracy: 0.0657\n",
            "Epoch 4/100\n",
            "194/194 [==============================] - 33s 172ms/step - loss: 4.9683 - accuracy: 0.0704\n",
            "Epoch 5/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.9048 - accuracy: 0.0837\n",
            "Epoch 6/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.8590 - accuracy: 0.0888\n",
            "Epoch 7/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.8231 - accuracy: 0.0966\n",
            "Epoch 8/100\n",
            "194/194 [==============================] - 34s 176ms/step - loss: 4.7884 - accuracy: 0.0992\n",
            "Epoch 9/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.7490 - accuracy: 0.1018\n",
            "Epoch 10/100\n",
            "194/194 [==============================] - 34s 174ms/step - loss: 4.7119 - accuracy: 0.1047\n",
            "Epoch 11/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.6816 - accuracy: 0.1045\n",
            "Epoch 12/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.6498 - accuracy: 0.1079\n",
            "Epoch 13/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.6155 - accuracy: 0.1113\n",
            "Epoch 14/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.5718 - accuracy: 0.1143\n",
            "Epoch 15/100\n",
            "194/194 [==============================] - 34s 175ms/step - loss: 4.5426 - accuracy: 0.1181\n",
            "Epoch 16/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.5140 - accuracy: 0.1147\n",
            "Epoch 17/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.4782 - accuracy: 0.1220\n",
            "Epoch 18/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.4489 - accuracy: 0.1262\n",
            "Epoch 19/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.4146 - accuracy: 0.1253\n",
            "Epoch 20/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 4.3895 - accuracy: 0.1300\n",
            "Epoch 21/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.3520 - accuracy: 0.1358\n",
            "Epoch 22/100\n",
            "194/194 [==============================] - 35s 178ms/step - loss: 4.3229 - accuracy: 0.1397\n",
            "Epoch 23/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.2900 - accuracy: 0.1363\n",
            "Epoch 24/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.2723 - accuracy: 0.1414\n",
            "Epoch 25/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.2337 - accuracy: 0.1489\n",
            "Epoch 26/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 4.2081 - accuracy: 0.1481\n",
            "Epoch 27/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 4.1778 - accuracy: 0.1507\n",
            "Epoch 28/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 4.1399 - accuracy: 0.1523\n",
            "Epoch 29/100\n",
            "194/194 [==============================] - 34s 177ms/step - loss: 4.1015 - accuracy: 0.1562\n",
            "Epoch 30/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 4.0648 - accuracy: 0.1589\n",
            "Epoch 31/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 4.0376 - accuracy: 0.1635\n",
            "Epoch 32/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.9970 - accuracy: 0.1677\n",
            "Epoch 33/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.9587 - accuracy: 0.1701\n",
            "Epoch 34/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.9275 - accuracy: 0.1733\n",
            "Epoch 35/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.8840 - accuracy: 0.1826\n",
            "Epoch 36/100\n",
            "194/194 [==============================] - 34s 177ms/step - loss: 3.8501 - accuracy: 0.1840\n",
            "Epoch 37/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.8221 - accuracy: 0.1834\n",
            "Epoch 38/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.8340 - accuracy: 0.1811\n",
            "Epoch 39/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.8154 - accuracy: 0.1876\n",
            "Epoch 40/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.7411 - accuracy: 0.1947\n",
            "Epoch 41/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.7149 - accuracy: 0.1994\n",
            "Epoch 42/100\n",
            "194/194 [==============================] - 32s 163ms/step - loss: 3.6644 - accuracy: 0.2080\n",
            "Epoch 43/100\n",
            "194/194 [==============================] - 34s 174ms/step - loss: 3.6292 - accuracy: 0.2141\n",
            "Epoch 44/100\n",
            "194/194 [==============================] - 32s 164ms/step - loss: 3.5777 - accuracy: 0.2193\n",
            "Epoch 45/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.5378 - accuracy: 0.2269\n",
            "Epoch 46/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 3.5321 - accuracy: 0.2267\n",
            "Epoch 47/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.4727 - accuracy: 0.2377\n",
            "Epoch 48/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.4286 - accuracy: 0.2463\n",
            "Epoch 49/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 3.4021 - accuracy: 0.2452\n",
            "Epoch 50/100\n",
            "194/194 [==============================] - 34s 175ms/step - loss: 3.3708 - accuracy: 0.2470\n",
            "Epoch 51/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.3002 - accuracy: 0.2667\n",
            "Epoch 52/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 3.2584 - accuracy: 0.2701\n",
            "Epoch 53/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.2250 - accuracy: 0.2751\n",
            "Epoch 54/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.1882 - accuracy: 0.2811\n",
            "Epoch 55/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.1459 - accuracy: 0.2933\n",
            "Epoch 56/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.1132 - accuracy: 0.3056\n",
            "Epoch 57/100\n",
            "194/194 [==============================] - 34s 176ms/step - loss: 3.1079 - accuracy: 0.2976\n",
            "Epoch 58/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.0569 - accuracy: 0.3146\n",
            "Epoch 59/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 3.0119 - accuracy: 0.3204\n",
            "Epoch 60/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.9574 - accuracy: 0.3335\n",
            "Epoch 61/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.9041 - accuracy: 0.3505\n",
            "Epoch 62/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.8673 - accuracy: 0.3565\n",
            "Epoch 63/100\n",
            "194/194 [==============================] - 34s 175ms/step - loss: 2.8593 - accuracy: 0.3658\n",
            "Epoch 64/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.7943 - accuracy: 0.3798\n",
            "Epoch 65/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.7744 - accuracy: 0.3849\n",
            "Epoch 66/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.7570 - accuracy: 0.3865\n",
            "Epoch 67/100\n",
            "194/194 [==============================] - 33s 169ms/step - loss: 2.7191 - accuracy: 0.4054\n",
            "Epoch 68/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.6672 - accuracy: 0.4132\n",
            "Epoch 69/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.6180 - accuracy: 0.4235\n",
            "Epoch 70/100\n",
            "194/194 [==============================] - 34s 175ms/step - loss: 2.5694 - accuracy: 0.4336\n",
            "Epoch 71/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.5368 - accuracy: 0.4438\n",
            "Epoch 72/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.5005 - accuracy: 0.4527\n",
            "Epoch 73/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.4716 - accuracy: 0.4622\n",
            "Epoch 74/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.4000 - accuracy: 0.4779\n",
            "Epoch 75/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.3674 - accuracy: 0.4892\n",
            "Epoch 76/100\n",
            "194/194 [==============================] - 33s 169ms/step - loss: 2.3567 - accuracy: 0.4886\n",
            "Epoch 77/100\n",
            "194/194 [==============================] - 34s 177ms/step - loss: 2.2963 - accuracy: 0.5064\n",
            "Epoch 78/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.2534 - accuracy: 0.5232\n",
            "Epoch 79/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 2.2622 - accuracy: 0.5206\n",
            "Epoch 80/100\n",
            "194/194 [==============================] - 32s 166ms/step - loss: 2.2268 - accuracy: 0.5320\n",
            "Epoch 81/100\n",
            "194/194 [==============================] - 32s 167ms/step - loss: 2.1883 - accuracy: 0.5299\n",
            "Epoch 82/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 2.1418 - accuracy: 0.5477\n",
            "Epoch 83/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 2.0853 - accuracy: 0.5614\n",
            "Epoch 84/100\n",
            "194/194 [==============================] - 34s 173ms/step - loss: 2.0626 - accuracy: 0.5693\n",
            "Epoch 85/100\n",
            "194/194 [==============================] - 32s 164ms/step - loss: 2.0464 - accuracy: 0.5745\n",
            "Epoch 86/100\n",
            "194/194 [==============================] - 32s 164ms/step - loss: 1.9886 - accuracy: 0.5857\n",
            "Epoch 87/100\n",
            "194/194 [==============================] - 32s 164ms/step - loss: 1.9689 - accuracy: 0.5965\n",
            "Epoch 88/100\n",
            "194/194 [==============================] - 32s 165ms/step - loss: 1.9394 - accuracy: 0.5990\n",
            "Epoch 89/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.9488 - accuracy: 0.5993\n",
            "Epoch 90/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.9709 - accuracy: 0.6004\n",
            "Epoch 91/100\n",
            "194/194 [==============================] - 34s 177ms/step - loss: 1.9390 - accuracy: 0.6003\n",
            "Epoch 92/100\n",
            "194/194 [==============================] - 33s 169ms/step - loss: 1.8604 - accuracy: 0.6240\n",
            "Epoch 93/100\n",
            "194/194 [==============================] - 33s 169ms/step - loss: 1.8087 - accuracy: 0.6372\n",
            "Epoch 94/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.7687 - accuracy: 0.6465\n",
            "Epoch 95/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.7504 - accuracy: 0.6532\n",
            "Epoch 96/100\n",
            "194/194 [==============================] - 33s 169ms/step - loss: 1.7458 - accuracy: 0.6597\n",
            "Epoch 97/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.7168 - accuracy: 0.6626\n",
            "Epoch 98/100\n",
            "194/194 [==============================] - 35s 178ms/step - loss: 1.7618 - accuracy: 0.6519\n",
            "Epoch 99/100\n",
            "194/194 [==============================] - 33s 168ms/step - loss: 1.7468 - accuracy: 0.6512\n",
            "Epoch 100/100\n",
            "194/194 [==============================] - 33s 170ms/step - loss: 1.7525 - accuracy: 0.6478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class myCallback(tf.keras.callbacks.Callback):\n",
        "# \tdef on_epoch_end(self, epoch, logs={}):\n",
        "# \t\tif(logs.get('accuracy')>0.93):\n",
        "# \t\t\tprint(\"\\nReached 93% accuracy so cancelling training!\")\n",
        "# \t\t\tself.model.stop_training = True\n",
        "\n",
        "# callbacks = myCallback()\n",
        "\n",
        "# history = model.fit(predictors, label, epochs=300, verbose=1, callbacks=[callbacks])\n"
      ],
      "metadata": {
        "id": "E82hDx5m5vtY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"water\"\n",
        "seed_text_two = \"olive oil\"\n",
        "seed_text_three = \"cinnamon\"\n",
        "seed_text_four = \"garlic\"\n",
        "seed_text_three = \"tequila\""
      ],
      "metadata": {
        "id": "oEP86Azk5UJm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "I9lux4Fkp39-"
      },
      "outputs": [],
      "source": [
        "# X_train, X_val, y_train, y_val= train_test_split(subset, subset['embeddings'], test_size= 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nOU6MPBvp39-"
      },
      "outputs": [],
      "source": [
        "# EMBEDDING_DIMENSIONS = 300\n",
        "\n",
        "# def construct_embedding_matrix(embeddings):\n",
        "#     num_ing = len(embeddings)\n",
        "\n",
        "#     # initialize a matrix of zeros\n",
        "#     embedding_matrix = np.zeros((num_ing, EMBEDDING_DIMENSIONS)) # each embedding has 300 dimensions\n",
        "\n",
        "#     next_row = 0\n",
        "\n",
        "#     for i in embeddings:\n",
        "#         v = embeddings.get(i)\n",
        "#         embedding_matrix[next_row] = v\n",
        "#         next_row+=1\n",
        "    \n",
        "#     return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cTeTLZ3Hp39-"
      },
      "outputs": [],
      "source": [
        "# embedding_matrix = construct_embedding_matrix(ing_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_REpMpM9p39_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd4da28-75ea-4192-883d-35ff29a64d09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.10600116,  0.04714949,  0.10841199, ..., -0.03144248,\n",
              "        -0.06629407, -0.1286629 ],\n",
              "       [-0.01582931,  0.09736368, -0.00062261, ..., -0.09226537,\n",
              "        -0.12149926, -0.12204846],\n",
              "       [-0.10132008,  0.03372396,  0.06472784, ..., -0.22692445,\n",
              "        -0.04366636, -0.20344618],\n",
              "       ...,\n",
              "       [-0.19128327,  0.17544127, -0.09963894, ..., -0.20900002,\n",
              "        -0.17799097, -0.1547064 ],\n",
              "       [ 0.02008764,  0.04900858, -0.26409724, ..., -0.19495088,\n",
              "        -0.16633987, -0.21576235],\n",
              "       [ 0.20899913, -0.15171458, -0.25460058, ..., -0.18800448,\n",
              "        -0.08664556, -0.07758268]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OXnnPfgFp39_"
      },
      "outputs": [],
      "source": [
        "# from keras import backend as K\n",
        "\n",
        "# def recall_m(y_true, y_pred):\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#     recall = true_positives / (possible_positives + K.epsilon())\n",
        "#     return recall\n",
        "\n",
        "# def precision_m(y_true, y_pred):\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     return precision\n",
        "\n",
        "# def f1_m(y_true, y_pred):\n",
        "#     precision = precision_m(y_true, y_pred)\n",
        "#     recall = recall_m(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FDtDQa9Ip39_"
      },
      "outputs": [],
      "source": [
        "# MAX_SEQUENCE_LENGTH= 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hIcIKcn9p39_"
      },
      "outputs": [],
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout\n",
        "# from keras.initializers import Constant\n",
        "# from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gAfuJ0z5p39_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fbad26-4698-4294-98bf-2721b536bbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# #print(type(X_train['start_ingredient']))\n",
        "# import numpy as np\n",
        "\n",
        "# x_train_array= X_train\n",
        "# x_train_array= np.asarray(x_train_array)\n",
        "\n",
        "# #print(type(v), v)\n",
        "\n",
        "# x_val_array= X_val\n",
        "# x_val_array= np.asarray(x_val_array)\n",
        "\n",
        "# y_train= np.asarray(y_train)\n",
        "# y_val= np.asarray(y_val)\n",
        "# print(type(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "emcKAVWYp3-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbededa4-18b7-4c72-ea2b-7a0a40d4ce0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 21) <dtype: 'float32'>\n",
            "(None, 832) <dtype: 'float32'>\n",
            "embedding (None, 21) float32\n",
            "bidirectional (None, 21, 240) float32\n",
            "dropout (None, 21, 300) float32\n",
            "lstm_1 (None, 21, 300) float32\n",
            "dense (None, 100) float32\n",
            "dense_1 (None, 416) float32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# [print(i.shape, i.dtype) for i in model.inputs]\n",
        "# [print(o.shape, o.dtype) for o in model.outputs]\n",
        "# [print(l.name, l.input_shape, l.dtype) for l in model.layers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FelVz_aUp3-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "c14481eb-4e1c-4477-c291-81d760db2ee6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a7071210a6f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     verbose=2)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ],
      "source": [
        "# model=Sequential()\n",
        "\n",
        "# embedding=Embedding(len(ing_embeddings), # number of unique tokens\n",
        "#                     EMBEDDING_DIMENSIONS, #number of features\n",
        "#                     embeddings_initializer=Constant(embedding_matrix), # initialize \n",
        "#                     input_length=MAX_SEQUENCE_LENGTH, \n",
        "#                     trainable=False)\n",
        "\n",
        "# model.add(embedding)\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# # compile the model\n",
        "# optimzer = Adam(clipvalue=0.5) # clip value to avoid the gradient exploding\n",
        "\n",
        "# model.compile(optimizer=optimzer, \n",
        "#               loss='binary_crossentropy', \n",
        "#               metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "# # fit the model\n",
        "# history = model.fit(x_train_array, y_train, \n",
        "#                     batch_size=32, \n",
        "#                     epochs=20, \n",
        "#                     validation_data=(x_val_array,y_val), \n",
        "#                     verbose=2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}