{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial: https://medium.com/analytics-vidhya/nlp-word-prediction-by-using-bidirectional-lstm-9c01c24b2725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>num_ingredients</th>\n",
       "      <th>start_ingredient</th>\n",
       "      <th>embedding_matrix_index_list</th>\n",
       "      <th>start_ingredient_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[beef, chicken_breast, cream_of_mushroom_soup,...</td>\n",
       "      <td>[[-0.33506337, 0.08803183, -0.24923237, 0.0341...</td>\n",
       "      <td>4</td>\n",
       "      <td>beef</td>\n",
       "      <td>[339, 1076, 1596, 5574]</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[peanut_butter, graham_cracker_crumb, butter, ...</td>\n",
       "      <td>[[-0.17900778, -0.039890748, -0.13081385, 0.30...</td>\n",
       "      <td>5</td>\n",
       "      <td>peanut_butter</td>\n",
       "      <td>[4375, 2733, 727, 4719, 1197]</td>\n",
       "      <td>4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[pineapple, condensed_milk, lemon, pecan, grah...</td>\n",
       "      <td>[[0.16943195, -0.31285393, -0.23219007, 0.1775...</td>\n",
       "      <td>5</td>\n",
       "      <td>pecan</td>\n",
       "      <td>[4519, 1412, 3479, 4401, 2735]</td>\n",
       "      <td>4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[chicken, flour, barbecue_sauce]</td>\n",
       "      <td>[[-0.22679155, 0.100118645, 0.10348936, 0.2072...</td>\n",
       "      <td>3</td>\n",
       "      <td>barbecue_sauce</td>\n",
       "      <td>[1066, 2242, 297]</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[pie_filling, pineapple, condensed_milk, lemon...</td>\n",
       "      <td>[[-0.021126581, 0.1452302, -0.09510492, -0.035...</td>\n",
       "      <td>4</td>\n",
       "      <td>pie_filling</td>\n",
       "      <td>[4487, 4519, 1412, 3499]</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  NER  \\\n",
       "1   [beef, chicken_breast, cream_of_mushroom_soup,...   \n",
       "4   [peanut_butter, graham_cracker_crumb, butter, ...   \n",
       "9   [pineapple, condensed_milk, lemon, pecan, grah...   \n",
       "12                   [chicken, flour, barbecue_sauce]   \n",
       "14  [pie_filling, pineapple, condensed_milk, lemon...   \n",
       "\n",
       "                                           embeddings  num_ingredients  \\\n",
       "1   [[-0.33506337, 0.08803183, -0.24923237, 0.0341...                4   \n",
       "4   [[-0.17900778, -0.039890748, -0.13081385, 0.30...                5   \n",
       "9   [[0.16943195, -0.31285393, -0.23219007, 0.1775...                5   \n",
       "12  [[-0.22679155, 0.100118645, 0.10348936, 0.2072...                3   \n",
       "14  [[-0.021126581, 0.1452302, -0.09510492, -0.035...                4   \n",
       "\n",
       "   start_ingredient     embedding_matrix_index_list  start_ingredient_index  \n",
       "1              beef         [339, 1076, 1596, 5574]                     339  \n",
       "4     peanut_butter   [4375, 2733, 727, 4719, 1197]                    4375  \n",
       "9             pecan  [4519, 1412, 3479, 4401, 2735]                    4401  \n",
       "12   barbecue_sauce               [1066, 2242, 297]                     297  \n",
       "14      pie_filling        [4487, 4519, 1412, 3499]                    4487  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open and load data from pickle file\n",
    "file = open('ner_embeddings.pkl', 'rb')\n",
    "\n",
    "# keys are strings for node_id, value is embedding\n",
    "embeddings_df = pickle.load(file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "max_num_ing = embeddings_df['num_ingredients'].max()\n",
    "\n",
    "# 44 is the max number of ingredients\n",
    "print(max_num_ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1076, 1596, 5574],\n",
       "       [   0,    0,    0, ...,  727, 4719, 1197],\n",
       "       [   0,    0,    0, ..., 3479, 4401, 2735],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  624, 3251, 2510],\n",
       "       [   0,    0,    0, ...,  727, 3822, 2998],\n",
       "       [   0,    0,    0, ..., 6471, 5228, 5601]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding the index lists to be 44 in length\n",
    "# using prepadding so that the last index will be the output\n",
    "input_sequences = np.array(pad_sequences(embeddings_df['embedding_matrix_index_list'], maxlen=max_num_ing, padding='pre'))\n",
    "\n",
    "input_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and load data from pickle file\n",
    "file = open('FlavorGraph_NodeEmbedding.pickle', 'rb')\n",
    "\n",
    "# keys are strings for node_id, value is embedding\n",
    "data = pickle.load(file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "# load dataframe of node_ids and ingredients\n",
    "df = pandas.read_csv('nodes_191120.csv')\n",
    "# just get the embeddings for ingredients\n",
    "ing_embeddings = {}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i, 'node_type'] == \"ingredient\":\n",
    "        # map the name of the ingredient to the embedding\n",
    "        ing_embeddings[df.loc[i, 'name']] = data[str(df.loc[i, 'node_id'])]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10600116,  0.04714949,  0.10841199, ..., -0.03144248,\n",
       "        -0.06629407, -0.1286629 ],\n",
       "       [-0.01582931,  0.09736368, -0.00062261, ..., -0.09226537,\n",
       "        -0.12149926, -0.12204846],\n",
       "       [-0.10132008,  0.03372396,  0.06472784, ..., -0.22692445,\n",
       "        -0.04366636, -0.20344618],\n",
       "       ...,\n",
       "       [-0.19128327,  0.17544127, -0.09963894, ..., -0.20900002,\n",
       "        -0.17799097, -0.1547064 ],\n",
       "       [ 0.02008764,  0.04900858, -0.26409724, ..., -0.19495088,\n",
       "        -0.16633987, -0.21576235],\n",
       "       [ 0.20899913, -0.15171458, -0.25460058, ..., -0.18800448,\n",
       "        -0.08664556, -0.07758268]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_embeddings = embeddings_df\n",
    "\n",
    "\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "\n",
    "# these indices are different from those in flavor graph\n",
    "matrix_ing_to_idx = {}\n",
    "idx_to_ing = {}\n",
    "\n",
    "def construct_embedding_matrix(embeddings):\n",
    "    num_ing = len(embeddings)\n",
    "\n",
    "    # initialize a matrix of zeros\n",
    "    # not adding a + 1 to num_ing because the data is cleaned such that only ones with valid ing are there\n",
    "    embedding_matrix = np.zeros((num_ing, EMBEDDING_DIMENSIONS)) # each embedding has 300 dimensions\n",
    "\n",
    "    next_row = 0\n",
    "\n",
    "    for i in embeddings:\n",
    "        v = embeddings.get(i)\n",
    "        embedding_matrix[next_row] = v\n",
    "        matrix_ing_to_idx[i] = next_row\n",
    "        idx_to_ing[next_row] = i\n",
    "        next_row+=1\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = construct_embedding_matrix(ing_embeddings)\n",
    "\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun up to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0, 4169, 1020, 2242,  727, 5228, 4296, 4963],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)\n",
    "input_subset = input_sequences[:100000]\n",
    "\n",
    "input_subset[99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will be the first 43, Y will be the last\n",
    "X, labels = input_subset[:,:-1],input_subset[:,-1]\n",
    "\n",
    "total_ing = 6653\n",
    "# converts to a classification problem, uses one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=total_ing)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6653"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorials for pre-trained embeddings\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/\n",
    "https://medium.com/analytics-vidhya/nlp-word-prediction-by-using-bidirectional-lstm-9c01c24b2725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "# use pre-trained embeddings\n",
    "embedding_layer = Embedding(len(embedding_matrix),\n",
    "                            EMBEDDING_DIMENSIONS,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_num_ing-1, # -1 because last idx is y\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_ing, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 383s 152ms/step - loss: 4.4247 - accuracy: 0.1567 - val_loss: 4.1453 - val_accuracy: 0.1900\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 372s 149ms/step - loss: 3.8566 - accuracy: 0.2235 - val_loss: 3.7995 - val_accuracy: 0.2382\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 371s 149ms/step - loss: 3.5249 - accuracy: 0.2648 - val_loss: 3.6311 - val_accuracy: 0.2686\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 358s 143ms/step - loss: 3.2885 - accuracy: 0.2961 - val_loss: 3.5567 - val_accuracy: 0.2856\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 382s 153ms/step - loss: 3.0909 - accuracy: 0.3227 - val_loss: 3.5140 - val_accuracy: 0.2939\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 379s 152ms/step - loss: 2.9097 - accuracy: 0.3481 - val_loss: 3.5030 - val_accuracy: 0.3025\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 373s 149ms/step - loss: 2.7366 - accuracy: 0.3705 - val_loss: 3.5149 - val_accuracy: 0.3061\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 369s 148ms/step - loss: 2.5712 - accuracy: 0.3947 - val_loss: 3.5385 - val_accuracy: 0.3082\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 370s 148ms/step - loss: 2.4072 - accuracy: 0.4243 - val_loss: 3.5974 - val_accuracy: 0.3074\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 400s 160ms/step - loss: 2.2488 - accuracy: 0.4505 - val_loss: 3.6640 - val_accuracy: 0.3072\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#             EarlyStopping(patience = 10)\n",
    "#             ]\n",
    "num_epochs = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 387s 155ms/step - loss: 3.4751 - accuracy: 0.3058 - val_loss: 3.3877 - val_accuracy: 0.3135\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 395s 158ms/step - loss: 3.1136 - accuracy: 0.3439 - val_loss: 3.3582 - val_accuracy: 0.3250\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 393s 157ms/step - loss: 2.8640 - accuracy: 0.3731 - val_loss: 3.3620 - val_accuracy: 0.3239\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 422s 169ms/step - loss: 2.6368 - accuracy: 0.4032 - val_loss: 3.3957 - val_accuracy: 0.3273\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 434s 173ms/step - loss: 2.4265 - accuracy: 0.4328 - val_loss: 3.4573 - val_accuracy: 0.3214\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 413s 165ms/step - loss: 2.2258 - accuracy: 0.4639 - val_loss: 3.5385 - val_accuracy: 0.3184\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 362s 145ms/step - loss: 2.0409 - accuracy: 0.4989 - val_loss: 3.6261 - val_accuracy: 0.3146\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 357s 143ms/step - loss: 1.8620 - accuracy: 0.5342 - val_loss: 3.7326 - val_accuracy: 0.3095\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 371s 149ms/step - loss: 1.7017 - accuracy: 0.5697 - val_loss: 3.8443 - val_accuracy: 0.3095\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 430s 172ms/step - loss: 1.5528 - accuracy: 0.6019 - val_loss: 3.9705 - val_accuracy: 0.3055\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 447s 179ms/step - loss: 3.5242 - accuracy: 0.3048 - val_loss: 3.3840 - val_accuracy: 0.3180\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 428s 171ms/step - loss: 3.0564 - accuracy: 0.3553 - val_loss: 3.3649 - val_accuracy: 0.3255\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 426s 170ms/step - loss: 2.7594 - accuracy: 0.3927 - val_loss: 3.3864 - val_accuracy: 0.3250\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 431s 172ms/step - loss: 2.4992 - accuracy: 0.4293 - val_loss: 3.4432 - val_accuracy: 0.3250\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 419s 168ms/step - loss: 2.2638 - accuracy: 0.4646 - val_loss: 3.5232 - val_accuracy: 0.3212\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 372s 149ms/step - loss: 2.0497 - accuracy: 0.5019 - val_loss: 3.6243 - val_accuracy: 0.3186\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 383s 153ms/step - loss: 1.8525 - accuracy: 0.5403 - val_loss: 3.7109 - val_accuracy: 0.3127\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 376s 150ms/step - loss: 1.6741 - accuracy: 0.5760 - val_loss: 3.8416 - val_accuracy: 0.3065\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 399s 159ms/step - loss: 1.5142 - accuracy: 0.6120 - val_loss: 3.9852 - val_accuracy: 0.2991\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 394s 158ms/step - loss: 1.3791 - accuracy: 0.6447 - val_loss: 4.1064 - val_accuracy: 0.2978\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 356s 142ms/step - loss: 4.0883 - accuracy: 0.2519 - val_loss: 3.9166 - val_accuracy: 0.2596\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 359s 144ms/step - loss: 3.5575 - accuracy: 0.3008 - val_loss: 3.8776 - val_accuracy: 0.2706\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 347s 139ms/step - loss: 3.2164 - accuracy: 0.3384 - val_loss: 3.8925 - val_accuracy: 0.2715\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 350s 140ms/step - loss: 2.9144 - accuracy: 0.3746 - val_loss: 3.9649 - val_accuracy: 0.2645\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 347s 139ms/step - loss: 2.6419 - accuracy: 0.4111 - val_loss: 4.0658 - val_accuracy: 0.2686\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 345s 138ms/step - loss: 2.3892 - accuracy: 0.4505 - val_loss: 4.1855 - val_accuracy: 0.2600\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 370s 148ms/step - loss: 2.1552 - accuracy: 0.4891 - val_loss: 4.3103 - val_accuracy: 0.2576\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 396s 159ms/step - loss: 1.9365 - accuracy: 0.5302 - val_loss: 4.5076 - val_accuracy: 0.2490\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 372s 149ms/step - loss: 1.7406 - accuracy: 0.5727 - val_loss: 4.6201 - val_accuracy: 0.2446\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 371s 148ms/step - loss: 1.5600 - accuracy: 0.6116 - val_loss: 4.8275 - val_accuracy: 0.2398\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 339s 136ms/step - loss: 4.9441 - accuracy: 0.1634 - val_loss: 4.7749 - val_accuracy: 0.1702\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 333s 133ms/step - loss: 4.4437 - accuracy: 0.1969 - val_loss: 4.7063 - val_accuracy: 0.1809\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 357s 143ms/step - loss: 4.1112 - accuracy: 0.2261 - val_loss: 4.6955 - val_accuracy: 0.1823\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 384s 153ms/step - loss: 3.7887 - accuracy: 0.2568 - val_loss: 4.7838 - val_accuracy: 0.1796\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 425s 170ms/step - loss: 3.4822 - accuracy: 0.2885 - val_loss: 4.9069 - val_accuracy: 0.1749\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 371s 148ms/step - loss: 3.1882 - accuracy: 0.3255 - val_loss: 5.0439 - val_accuracy: 0.1680\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 375s 150ms/step - loss: 2.9018 - accuracy: 0.3653 - val_loss: 5.1865 - val_accuracy: 0.1629\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 366s 146ms/step - loss: 2.6352 - accuracy: 0.4065 - val_loss: 5.4328 - val_accuracy: 0.1545\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 363s 145ms/step - loss: 2.3811 - accuracy: 0.4527 - val_loss: 5.6089 - val_accuracy: 0.1480\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 354s 142ms/step - loss: 2.1469 - accuracy: 0.4978 - val_loss: 5.7948 - val_accuracy: 0.1453\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 343s 137ms/step - loss: 4.7913 - accuracy: 0.1656 - val_loss: 4.5866 - val_accuracy: 0.1775\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 311s 124ms/step - loss: 4.3502 - accuracy: 0.1984 - val_loss: 4.5299 - val_accuracy: 0.1845\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 323s 129ms/step - loss: 4.0308 - accuracy: 0.2299 - val_loss: 4.5522 - val_accuracy: 0.1847\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 336s 134ms/step - loss: 3.7145 - accuracy: 0.2626 - val_loss: 4.6431 - val_accuracy: 0.1831\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 355s 142ms/step - loss: 3.4007 - accuracy: 0.2980 - val_loss: 4.7806 - val_accuracy: 0.1766\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 341s 136ms/step - loss: 3.1020 - accuracy: 0.3378 - val_loss: 4.9082 - val_accuracy: 0.1740\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 355s 142ms/step - loss: 2.8165 - accuracy: 0.3772 - val_loss: 5.0666 - val_accuracy: 0.1680\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 354s 142ms/step - loss: 2.5430 - accuracy: 0.4244 - val_loss: 5.2851 - val_accuracy: 0.1626\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 356s 142ms/step - loss: 2.2933 - accuracy: 0.4719 - val_loss: 5.4649 - val_accuracy: 0.1521\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 352s 141ms/step - loss: 2.0612 - accuracy: 0.5177 - val_loss: 5.6929 - val_accuracy: 0.1514\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 335s 134ms/step - loss: 4.8410 - accuracy: 0.1595 - val_loss: 4.6573 - val_accuracy: 0.1739\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 356s 143ms/step - loss: 4.4091 - accuracy: 0.1902 - val_loss: 4.6069 - val_accuracy: 0.1793\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 361s 144ms/step - loss: 4.0804 - accuracy: 0.2202 - val_loss: 4.6334 - val_accuracy: 0.1801\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 343s 137ms/step - loss: 3.7544 - accuracy: 0.2524 - val_loss: 4.7242 - val_accuracy: 0.1738\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 346s 138ms/step - loss: 3.4377 - accuracy: 0.2882 - val_loss: 4.8495 - val_accuracy: 0.1697\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 346s 138ms/step - loss: 3.1288 - accuracy: 0.3304 - val_loss: 5.0385 - val_accuracy: 0.1629\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 357s 143ms/step - loss: 2.8342 - accuracy: 0.3739 - val_loss: 5.2117 - val_accuracy: 0.1576\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 359s 143ms/step - loss: 2.5558 - accuracy: 0.4219 - val_loss: 5.4508 - val_accuracy: 0.1496\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 367s 147ms/step - loss: 2.3022 - accuracy: 0.4694 - val_loss: 5.6503 - val_accuracy: 0.1470\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 365s 146ms/step - loss: 2.0688 - accuracy: 0.5171 - val_loss: 5.8876 - val_accuracy: 0.1358\n",
      "Epoch 1/10\n",
      "1511/1511 [==============================] - 221s 145ms/step - loss: 4.8328 - accuracy: 0.1587 - val_loss: 4.7478 - val_accuracy: 0.1649\n",
      "Epoch 2/10\n",
      "1511/1511 [==============================] - 212s 141ms/step - loss: 4.3801 - accuracy: 0.1957 - val_loss: 4.7297 - val_accuracy: 0.1697\n",
      "Epoch 3/10\n",
      "1511/1511 [==============================] - 215s 142ms/step - loss: 4.0226 - accuracy: 0.2303 - val_loss: 4.8044 - val_accuracy: 0.1666\n",
      "Epoch 4/10\n",
      "1511/1511 [==============================] - 211s 140ms/step - loss: 3.6358 - accuracy: 0.2701 - val_loss: 4.9341 - val_accuracy: 0.1641\n",
      "Epoch 5/10\n",
      "1511/1511 [==============================] - 211s 140ms/step - loss: 3.2364 - accuracy: 0.3235 - val_loss: 5.0932 - val_accuracy: 0.1543\n",
      "Epoch 6/10\n",
      "1511/1511 [==============================] - 210s 139ms/step - loss: 2.8452 - accuracy: 0.3812 - val_loss: 5.3916 - val_accuracy: 0.1489\n",
      "Epoch 7/10\n",
      "1511/1511 [==============================] - 208s 138ms/step - loss: 2.4762 - accuracy: 0.4438 - val_loss: 5.6283 - val_accuracy: 0.1414\n",
      "Epoch 8/10\n",
      "1511/1511 [==============================] - 209s 138ms/step - loss: 2.1363 - accuracy: 0.5106 - val_loss: 5.9161 - val_accuracy: 0.1320\n",
      "Epoch 9/10\n",
      "1511/1511 [==============================] - 212s 140ms/step - loss: 1.8363 - accuracy: 0.5765 - val_loss: 6.1731 - val_accuracy: 0.1260\n",
      "Epoch 10/10\n",
      "1511/1511 [==============================] - 209s 138ms/step - loss: 1.5678 - accuracy: 0.6353 - val_loss: 6.4733 - val_accuracy: 0.1207\n"
     ]
    }
   ],
   "source": [
    "i = 100000\n",
    "while i < len(input_sequences):\n",
    "    input_subset = input_sequences[i: i+100000]\n",
    "    # X will be the first 43, Y will be the last\n",
    "    X, labels = input_subset[:,:-1],input_subset[:,-1]\n",
    "\n",
    "    total_ing = 6653\n",
    "    # converts to a classification problem, uses one-hot encoding\n",
    "    y = tf.keras.utils.to_categorical(labels, num_classes=total_ing)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.20)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))\n",
    "    i+=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 31s 125ms/step - loss: 4.7998 - accuracy: 0.1593 - val_loss: 4.3856 - val_accuracy: 0.1900\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 3.8000 - accuracy: 0.2442 - val_loss: 4.3615 - val_accuracy: 0.2075\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 3.2778 - accuracy: 0.3223 - val_loss: 4.4096 - val_accuracy: 0.2055\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 2.7856 - accuracy: 0.3909 - val_loss: 4.5512 - val_accuracy: 0.2125\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 2.3409 - accuracy: 0.4690 - val_loss: 4.6879 - val_accuracy: 0.2045\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 29s 118ms/step - loss: 1.9157 - accuracy: 0.5571 - val_loss: 4.8542 - val_accuracy: 0.1995\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 31s 123ms/step - loss: 1.5473 - accuracy: 0.6404 - val_loss: 4.9708 - val_accuracy: 0.2020\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 30s 119ms/step - loss: 1.2141 - accuracy: 0.7306 - val_loss: 5.2062 - val_accuracy: 0.1990\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 30s 119ms/step - loss: 0.9315 - accuracy: 0.8035 - val_loss: 5.3693 - val_accuracy: 0.1895\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 0.6985 - accuracy: 0.8679 - val_loss: 5.5211 - val_accuracy: 0.1965\n"
     ]
    }
   ],
   "source": [
    "# input_subset = input_sequences[40000: 50000]\n",
    "# # X will be the first 43, Y will be the last\n",
    "# X, labels = input_subset[:,:-1],input_subset[:,-1]\n",
    "\n",
    "# total_ing = 6653\n",
    "# # converts to a classification problem, uses one-hot encoding\n",
    "# y = tf.keras.utils.to_categorical(labels, num_classes=total_ing)\n",
    "\n",
    "# # split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.20)\n",
    "    \n",
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ingredients_model_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ingredients_model_2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"ingredients_model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(input_sequences)\n",
    "\n",
    "input_subset = input_sequences[540000: 550000]\n",
    "# X will be the first 43, Y will be the last\n",
    "X, labels = input_subset[:,:-1],input_subset[:,-1]\n",
    "\n",
    "total_ing = 6653\n",
    "# converts to a classification problem, uses one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=total_ing)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model should be saved to ingredients_model, so you can rerun everything without retraining\n",
    "pre_trained = keras.models.load_model(\"ingredients_model_2\")\n",
    "\n",
    "ingredients_model = Sequential()\n",
    "\n",
    "for layer in pre_trained.layers[:-1]: # this is where I changed your code\n",
    "    ingredients_model.add(layer)    \n",
    "\n",
    "# Freeze the layers \n",
    "for layer in ingredients_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a scale from 1 to 99, how adventurous are you feeling? 50\n",
      "Epoch 1/10\n",
      "250/250 [==============================] - 22s 75ms/step - loss: 9.2294 - accuracy: 0.1908 - val_loss: 8.8003 - val_accuracy: 0.1930\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 8.4903 - accuracy: 0.2051 - val_loss: 8.5283 - val_accuracy: 0.1945\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 7.9723 - accuracy: 0.2148 - val_loss: 8.3746 - val_accuracy: 0.1940\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 7.5400 - accuracy: 0.2214 - val_loss: 8.2466 - val_accuracy: 0.1955\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 7.1546 - accuracy: 0.2304 - val_loss: 8.1445 - val_accuracy: 0.1925\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 6.8031 - accuracy: 0.2380 - val_loss: 8.0651 - val_accuracy: 0.1915\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 6.4788 - accuracy: 0.2470 - val_loss: 7.9930 - val_accuracy: 0.1940\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 6.1778 - accuracy: 0.2545 - val_loss: 7.9359 - val_accuracy: 0.1915\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 5.8935 - accuracy: 0.2640 - val_loss: 7.8828 - val_accuracy: 0.1900\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 5.6244 - accuracy: 0.2724 - val_loss: 7.8406 - val_accuracy: 0.1845\n"
     ]
    }
   ],
   "source": [
    "# get user's input for temperature\n",
    "temp = float(input(\"On a scale from 1 to 99, how adventurous are you feeling? \"))\n",
    "temp = temp/100.0\n",
    "# flipping because higher temps should be more conservative\n",
    "temp = (temp * -1) + 1\n",
    "# print(temp)\n",
    "# add temp\n",
    "ingredients_model.add(Lambda(lambda x: x / temp))\n",
    "ingredients_model.add(pre_trained.layers[-1])\n",
    "\n",
    "ingredients_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = ingredients_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without the temp\n",
    "# ingredients_model = keras.models.load_model(\"ingredients_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_ing_list(start_ing_idx, num_ing):\n",
    "#     token_list = []\n",
    "#     token_list.append(start_ing_idx)\n",
    "\n",
    "#     # now token_list should be 2D\n",
    "#     save = []\n",
    "#     # recommended_ing = []\n",
    "\n",
    "#     for _ in range(num_ing-1):\n",
    "#         token_list = pad_sequences([token_list], maxlen=max_num_ing-1, padding='pre', truncating='pre')\n",
    "\n",
    "#         predicted = np.argmax(ingredients_model.predict(token_list), axis=-1)\n",
    "#         # print(predicted)\n",
    "#         # predicted_idx = np.argmax([predicted], axis=1)\n",
    "#         # save =predicted_idx\n",
    "#         token_list = np.append(token_list[0], predicted[0])\n",
    "#         # print(token_list)\n",
    "#         # token_list = pad_sequences(token_list, maxlen=max_num_ing-1, padding='pre', truncating='pre')\n",
    "\n",
    "#     return token_list\n",
    "\n",
    "def predict_ing_list(token_list, num_ing):\n",
    "    save = []\n",
    "\n",
    "    for _ in range(num_ing-1):\n",
    "        # after padding token_list will be 2D\n",
    "        token_list = pad_sequences([token_list], maxlen=max_num_ing-1, padding='pre', truncating='pre')\n",
    "\n",
    "        predicted = np.argmax(ingredients_model.predict(token_list), axis=-1)\n",
    "        \n",
    "        # back to 1D\n",
    "        token_list = np.append(token_list[0], predicted[0])\n",
    "\n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose your start ingredient(s), separated by a space: carrot cinnamon sugar\n",
      "How many ingredients do you want? 14\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "carrot\n",
      "cinnamon\n",
      "sugar\n",
      "butter\n",
      "cheese\n",
      "wine\n",
      "egg\n",
      "milk\n",
      "salt\n",
      "milk\n",
      "butter\n",
      "sugar\n",
      "flour\n",
      "vanilla\n",
      "nut\n",
      "maple_syrup\n"
     ]
    }
   ],
   "source": [
    "# start_ing = input(\"Choose your start ingredient: \")\n",
    "# start_ing_idx = matrix_ing_to_idx.get(start_ing, -1)\n",
    "# num_ing = int(input(\"How many ingredients do you want? \"))\n",
    "# recipe_ingr= []\n",
    "\n",
    "# token_list = []\n",
    "# if start_ing_idx != -1 and num_ing > 1:\n",
    "#     token_list = predict_ing_list(start_ing_idx, num_ing)\n",
    "# elif num_ing <= 1:\n",
    "#     print(\"Too few ingredients--please select a higher number.\")\n",
    "# else:\n",
    "#     print(\"Sorry, your ingredient isn't in our list. Try a different spelling or ingredient!\")\n",
    "    \n",
    "# for idx in token_list:\n",
    "#     if idx != 0:\n",
    "#         recipe_ingr.append(idx_to_ing[idx])\n",
    "#         print(idx_to_ing[idx])\n",
    "\n",
    "\n",
    "start_ings = input(\"Choose your start ingredient(s), separated by a space: \")\n",
    "start_ing_list = start_ings.split(\" \")\n",
    "num_ing = int(input(\"How many ingredients do you want? \"))\n",
    "recipe_ingr = []\n",
    "\n",
    "token_list = []\n",
    "for ing in start_ing_list:\n",
    "    # convert to idx\n",
    "    ing_idx = matrix_ing_to_idx.get(ing, -1)\n",
    "    if ing_idx != -1:\n",
    "        token_list.append(ing_idx)\n",
    "    else:\n",
    "        print(\"Sorry, \"+ing+\" isn't in our ingredients list\")\n",
    "\n",
    "if len(token_list) > 0 and num_ing > 1:\n",
    "    token_list = predict_ing_list(token_list, num_ing)\n",
    "elif num_ing <= 1:\n",
    "    print(\"Too few ingredients--please select a higher number.\")\n",
    "elif num_ing < len(token_list):\n",
    "    print(\"You gave us more start ingredients than you want in your final recipe. Please try again.\")\n",
    "elif len(token_list) < 1:\n",
    "    print(\"Sorry, none of your ingredients are in our list. Try a different spelling or ingredient!\")\n",
    "    \n",
    "for idx in token_list:\n",
    "    if idx != 0:\n",
    "        recipe_ingr.append(idx_to_ing[idx])\n",
    "        print(idx_to_ing[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_Completion(texts):\n",
    "## Call the API key under your account (in a secure way)\n",
    "  openai.api_key = \"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\"\n",
    "  response = openai.Completion.create(\n",
    "  engine=\"text-davinci-002\",\n",
    "  prompt =  texts,\n",
    "  temperature = 0.6,\n",
    "  top_p = 1,\n",
    "  max_tokens = 1000,\n",
    "  frequency_penalty = 0,\n",
    "  presence_penalty = 0\n",
    "  )\n",
    "  return print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recipe= \"Provide a cooking recipe based on the following ingredients:\" \n",
    "recipe_ingr= set(recipe_ingr)\n",
    "recipe_ingr= list(recipe_ingr)\n",
    "\n",
    "for i in recipe_ingr: \n",
    "  add_ingr= \"\\n \\n\" + i\n",
    "  if len(recipe_ingr)== recipe_ingr.index(i): \n",
    "    add_ingr= add_ingr + \" \\\\\"\n",
    "  sample_recipe= sample_recipe + add_ingr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maple Carrot Cake Recipe\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "1 cup sugar\n",
      "1 teaspoon salt\n",
      "1 cup maple syrup\n",
      "1 cup grated carrot\n",
      "1 cup milk\n",
      "1 cup chopped nuts\n",
      "3 eggs\n",
      "1 cup flour\n",
      "1 cup grated cheese\n",
      "1 teaspoon cinnamon\n",
      "1/2 cup butter\n",
      "1 teaspoon vanilla\n",
      "1/4 cup wine\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat oven to 350 degrees F. Grease and flour a 9x13 inch baking pan.\n",
      "\n",
      "2. In a large bowl, combine the sugar, salt, maple syrup, carrot, milk, nuts, eggs, flour, cheese, cinnamon, butter, vanilla, and wine. Mix well.\n",
      "\n",
      "3. Pour into the prepared pan.\n",
      "\n",
      "4. Bake for 45 minutes to 1 hour, or until a toothpick inserted into the center comes out clean.\n"
     ]
    }
   ],
   "source": [
    "GPT_Completion(sample_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
